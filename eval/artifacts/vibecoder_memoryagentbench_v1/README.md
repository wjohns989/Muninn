# vibecoder_memoryagentbench_v1 Artifacts

Canonical benchmark artifact bundle for Muninn's preset-driven eval gate.

## Files

- `dataset.jsonl`: query ground truth with `query_id`, `track`, `relevant_ids`.
- `baseline_predictions.jsonl`: baseline ranked outputs and `latency_ms`.
- `baseline_report.json`: canonical report generated by `python -m eval.run --preset vibecoder_memoryagentbench_v1`.
- `manifest.json`: SHA-256 checksums + dataset contract metadata.

## Integrity + Reproducibility Verification

```bash
python -m eval.artifacts verify --preset vibecoder_memoryagentbench_v1
```

Verification checks:

1. SHA-256 checksum integrity for canonical files.
2. Dataset contract validity (row count, unique query IDs, non-empty relevance labels, track counts).
3. Baseline report reproducibility from canonical dataset + predictions under current eval code.
