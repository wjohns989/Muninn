{"question_id": "lme_syn_001", "question_type": "single-session-qa", "question": "What programming language does the user prefer for backend services?", "expected_answer": "Python", "question_date": "2025-06-15", "sessions": [{"session_id": "session_001_a", "date": "2025-06-10", "conversation": [{"role": "user", "content": "I need to decide on a backend language for our new microservice.", "turn_id": 1}, {"role": "assistant", "content": "What are your team's existing skills?", "turn_id": 2}, {"role": "user", "content": "Our team has strong Python expertise and we use FastAPI extensively. Python is definitely our preferred backend language for all services.", "turn_id": 3}, {"role": "assistant", "content": "Python with FastAPI is an excellent choice for microservices. Great performance and easy async support.", "turn_id": 4}]}]}
{"question_id": "lme_syn_002", "question_type": "single-session-qa", "question": "What is the name of the user's AI memory project?", "expected_answer": "Muninn", "question_date": "2025-06-15", "sessions": [{"session_id": "session_002_a", "date": "2025-06-11", "conversation": [{"role": "user", "content": "Let me tell you about the project I'm building.", "turn_id": 1}, {"role": "assistant", "content": "I'd love to hear about it.", "turn_id": 2}, {"role": "user", "content": "It's called Muninn — named after Odin's raven of memory in Norse mythology. It's an MCP-based persistent memory system for AI agents.", "turn_id": 3}, {"role": "assistant", "content": "Fascinating name. Muninn embodies exactly what a memory system should be — always gathering and retaining.", "turn_id": 4}]}]}
{"question_id": "lme_syn_003", "question_type": "single-session-qa", "question": "What database does the user use for vector storage?", "expected_answer": "Qdrant", "question_date": "2025-06-15", "sessions": [{"session_id": "session_003_a", "date": "2025-06-12", "conversation": [{"role": "user", "content": "I'm evaluating vector databases for semantic search.", "turn_id": 1}, {"role": "assistant", "content": "What are your requirements?", "turn_id": 2}, {"role": "user", "content": "We need local-first, no cloud dependencies. After testing Chroma, Weaviate, and Qdrant, we chose Qdrant for vector storage because of its filtering performance and Rust performance.", "turn_id": 3}, {"role": "assistant", "content": "Qdrant is a solid choice — excellent filtering and high throughput.", "turn_id": 4}]}]}
{"question_id": "lme_syn_004", "question_type": "single-session-qa", "question": "What port does the Muninn server run on?", "expected_answer": "42069", "question_date": "2025-06-15", "sessions": [{"session_id": "session_004_a", "date": "2025-06-12", "conversation": [{"role": "user", "content": "What port should I configure for the Muninn server?", "turn_id": 1}, {"role": "assistant", "content": "The default configuration uses a specific port.", "turn_id": 2}, {"role": "user", "content": "We set it to port 42069 in our deployment. All MCP wrappers connect to http://localhost:42069.", "turn_id": 3}, {"role": "assistant", "content": "Port 42069 noted. I'll update the documentation.", "turn_id": 4}]}]}
{"question_id": "lme_syn_005", "question_type": "single-session-qa", "question": "What graph database does the user use for memory chains?", "expected_answer": "KuzuDB", "question_date": "2025-06-15", "sessions": [{"session_id": "session_005_a", "date": "2025-06-13", "conversation": [{"role": "user", "content": "I need a graph database for storing causal memory relationships.", "turn_id": 1}, {"role": "assistant", "content": "What kind of graph relationships are you modeling?", "turn_id": 2}, {"role": "user", "content": "PRECEDES and CAUSES relations between memories. We evaluated Neo4j and KuzuDB. We went with KuzuDB for memory chains because it's embeddable, has no server process, and is open source.", "turn_id": 3}, {"role": "assistant", "content": "KuzuDB is excellent for embedded graph workloads. Great choice.", "turn_id": 4}]}]}
{"question_id": "lme_syn_006", "question_type": "single-session-qa", "question": "What authentication scheme does Muninn use for its REST API?", "expected_answer": "Bearer token", "question_date": "2025-06-15", "sessions": [{"session_id": "session_006_a", "date": "2025-06-13", "conversation": [{"role": "user", "content": "How is the Muninn server authentication implemented?", "turn_id": 1}, {"role": "assistant", "content": "What security model do you have in mind?", "turn_id": 2}, {"role": "user", "content": "We implemented Bearer token authentication. Every request must include an Authorization: Bearer header with the MUNINN_AUTH_TOKEN value. The token is stored in a .muninn_token file.", "turn_id": 3}, {"role": "assistant", "content": "Bearer token auth is clean and stateless. Solid approach.", "turn_id": 4}]}]}
{"question_id": "lme_syn_007", "question_type": "single-session-qa", "question": "What is the user's company name?", "expected_answer": "Antigravity Labs", "question_date": "2025-06-15", "sessions": [{"session_id": "session_007_a", "date": "2025-06-14", "conversation": [{"role": "user", "content": "I should introduce myself properly.", "turn_id": 1}, {"role": "assistant", "content": "Please do!", "turn_id": 2}, {"role": "user", "content": "I work at Antigravity Labs. We build developer tools and AI infrastructure. The Muninn project is our flagship open-source contribution.", "turn_id": 3}, {"role": "assistant", "content": "Antigravity Labs sounds like an exciting place to work. Developer tools and AI infrastructure are high-impact areas.", "turn_id": 4}]}]}
{"question_id": "lme_syn_008", "question_type": "single-session-qa", "question": "What license is Muninn published under?", "expected_answer": "Apache-2.0", "question_date": "2025-06-15", "sessions": [{"session_id": "session_008_a", "date": "2025-06-14", "conversation": [{"role": "user", "content": "We need to decide on an open source license for Muninn.", "turn_id": 1}, {"role": "assistant", "content": "What are your goals for the project?", "turn_id": 2}, {"role": "user", "content": "We want permissive but with patent protection. We chose the Apache-2.0 license for Muninn. It allows commercial use, modification, and distribution while providing patent rights.", "turn_id": 3}, {"role": "assistant", "content": "Apache-2.0 is widely adopted and respected in the enterprise open source world.", "turn_id": 4}]}]}
{"question_id": "lme_syn_009", "question_type": "single-session-qa", "question": "What embedding model does Muninn use by default?", "expected_answer": "fastembed", "question_date": "2025-06-15", "sessions": [{"session_id": "session_009_a", "date": "2025-06-14", "conversation": [{"role": "user", "content": "What embedding model should I use for the vector store?", "turn_id": 1}, {"role": "assistant", "content": "Several options exist from OpenAI to local models.", "turn_id": 2}, {"role": "user", "content": "For local-first operation we use fastembed as the default embedding library. It runs entirely on CPU without API keys, and the BAAI/bge-small-en-v1.5 model is the default.", "turn_id": 3}, {"role": "assistant", "content": "fastembed is a smart choice for local-first — ONNX-optimized and fast on CPU.", "turn_id": 4}]}]}
{"question_id": "lme_syn_010", "question_type": "single-session-qa", "question": "What MCP protocol version does Muninn target?", "expected_answer": "2025-11-05", "question_date": "2025-06-15", "sessions": [{"session_id": "session_010_a", "date": "2025-06-14", "conversation": [{"role": "user", "content": "Which MCP protocol version should we implement?", "turn_id": 1}, {"role": "assistant", "content": "The spec has been evolving rapidly.", "turn_id": 2}, {"role": "user", "content": "We target the 2025-11-05 MCP protocol version. Our handlers.py returns this as the protocolVersion in the initialize response.", "turn_id": 3}, {"role": "assistant", "content": "Good. The 2025-11-05 spec is the current stable baseline.", "turn_id": 4}]}]}
{"question_id": "lme_syn_011", "question_type": "multi-session-qa", "question": "What database migration strategy does the user use to add new columns?", "expected_answer": "idempotent ALTER TABLE via _ensure_column_exists", "question_date": "2025-07-01", "sessions": [{"session_id": "session_011_a", "date": "2025-06-20", "conversation": [{"role": "user", "content": "I need to add a new column to existing SQLite databases without breaking old installs.", "turn_id": 1}, {"role": "assistant", "content": "What's the schema change you need?", "turn_id": 2}, {"role": "user", "content": "Adding a scope column. I'll use an idempotent _ensure_column_exists pattern: try ALTER TABLE, catch OperationalError if column already exists. This is safe to run on every startup.", "turn_id": 3}]}, {"session_id": "session_011_b", "date": "2025-06-22", "conversation": [{"role": "user", "content": "The _ensure_column_exists migration worked perfectly. Now adding indexes after the column is confirmed present.", "turn_id": 1}, {"role": "assistant", "content": "Good. Idempotent migrations are critical for upgrade safety.", "turn_id": 2}]}]}
{"question_id": "lme_syn_012", "question_type": "multi-session-qa", "question": "What was the bug in get_vector() that caused retrieval to always return None?", "expected_answer": "memory_id was passed as raw string instead of UUID5 to Qdrant retrieve", "question_date": "2025-07-01", "sessions": [{"session_id": "session_012_a", "date": "2025-06-18", "conversation": [{"role": "user", "content": "The vector store get_vector() always returns None even when the memory exists.", "turn_id": 1}, {"role": "assistant", "content": "Let me check the retrieve call.", "turn_id": 2}, {"role": "user", "content": "Found it: client.retrieve() was passed the raw memory_id string but Qdrant stores points under UUID5(memory_id). So every lookup missed.", "turn_id": 3}]}, {"session_id": "session_012_b", "date": "2025-06-19", "conversation": [{"role": "user", "content": "Fixed the UUID bug in both get_vector and get_vectors. Now converting memory_id to UUID5 before retrieve, then mapping back in the results.", "turn_id": 1}, {"role": "assistant", "content": "Critical fix. UUID5 conversion needs to be consistent at every Qdrant boundary.", "turn_id": 2}]}]}
{"question_id": "lme_syn_013", "question_type": "multi-session-qa", "question": "What test framework does the user use and what is the target test count for Phase 16?", "expected_answer": "pytest, 788 tests", "question_date": "2025-07-01", "sessions": [{"session_id": "session_013_a", "date": "2025-06-25", "conversation": [{"role": "user", "content": "What test framework should we standardize on?", "turn_id": 1}, {"role": "assistant", "content": "Several options available for Python.", "turn_id": 2}, {"role": "user", "content": "We use pytest for all testing. It's cleaner than unittest and the fixture model is much better for our stateful server tests.", "turn_id": 3}]}, {"session_id": "session_013_b", "date": "2025-06-28", "conversation": [{"role": "user", "content": "Phase 16 complete. We now have 788 tests passing with pytest. 0 failures. That's 727 from Phase 15 plus 61 new Phase 16 tests.", "turn_id": 1}, {"role": "assistant", "content": "788 passing tests with zero failures is excellent quality coverage.", "turn_id": 2}]}]}
{"question_id": "lme_syn_014", "question_type": "multi-session-qa", "question": "What signing algorithm is used for the SOTA+ verdict promotion signature?", "expected_answer": "HMAC-SHA256", "question_date": "2025-07-01", "sessions": [{"session_id": "session_014_a", "date": "2025-06-26", "conversation": [{"role": "user", "content": "We need a tamper-evident signature on the SOTA+ verdict artifact.", "turn_id": 1}, {"role": "assistant", "content": "What trust model do you need?", "turn_id": 2}, {"role": "user", "content": "The verdict must be self-verifying offline. We're using HMAC-SHA256 over a canonical JSON payload containing run_id, passed, commit_sha, and input_file_hashes.", "turn_id": 3}]}, {"session_id": "session_014_b", "date": "2025-06-27", "conversation": [{"role": "user", "content": "HMAC-SHA256 signing implemented. The promotion_signature field is formatted as hmac-sha256=<64 hex chars>. Null when no signing key is provided.", "turn_id": 1}, {"role": "assistant", "content": "HMAC-SHA256 provides strong integrity guarantees and is easy to verify independently.", "turn_id": 2}]}]}
{"question_id": "lme_syn_015", "question_type": "multi-session-qa", "question": "What minimum nDCG@10 threshold is required for the LongMemEval gate to pass?", "expected_answer": "0.60", "question_date": "2025-07-01", "sessions": [{"session_id": "session_015_a", "date": "2025-06-29", "conversation": [{"role": "user", "content": "What LongMemEval threshold should we set for the SOTA+ gate?", "turn_id": 1}, {"role": "assistant", "content": "What baseline does the paper report?", "turn_id": 2}, {"role": "user", "content": "The paper shows strong systems achieve nDCG@10 around 0.65-0.75. We'll set the minimum LongMemEval nDCG@10 threshold at 0.60 as the gate floor — that's below typical SOTA but above random baseline.", "turn_id": 3}]}, {"session_id": "session_015_b", "date": "2025-06-30", "conversation": [{"role": "user", "content": "The LongMemEval gate is now a hard gate in overall_passed. If ndcg_at_10 < 0.60 and --require-longmemeval is set, overall_passed becomes False.", "turn_id": 1}, {"role": "assistant", "content": "Good. Hard gates prevent silent regression as the codebase evolves.", "turn_id": 2}]}]}
{"question_id": "lme_syn_016", "question_type": "multi-session-qa", "question": "What feature flag controls whether project-scope memories can leak into cross-project fallback searches?", "expected_answer": "MUNINN_PROJECT_SCOPE_STRICT", "question_date": "2025-07-01", "sessions": [{"session_id": "session_016_a", "date": "2025-06-17", "conversation": [{"role": "user", "content": "When searching, project-specific memories are leaking into unrelated project searches.", "turn_id": 1}, {"role": "assistant", "content": "How does the fallback search currently work?", "turn_id": 2}, {"role": "user", "content": "The MUNINN_MCP_SEARCH_PROJECT_FALLBACK retry was returning scope=project memories from other repos. We need MUNINN_PROJECT_SCOPE_STRICT to completely disable the fallback.", "turn_id": 3}]}, {"session_id": "session_016_b", "date": "2025-06-18", "conversation": [{"role": "user", "content": "The MUNINN_PROJECT_SCOPE_STRICT feature flag is now implemented. When enabled, the fallback never runs — zero cross-project memory leakage guaranteed.", "turn_id": 1}, {"role": "assistant", "content": "MUNINN_PROJECT_SCOPE_STRICT provides maximum isolation for multi-project environments.", "turn_id": 2}]}]}
{"question_id": "lme_syn_017", "question_type": "multi-session-qa", "question": "What was the fix for the auth token propagation bug in lifecycle.py?", "expected_answer": "start_server() now reads MUNINN_AUTH_TOKEN and passes it explicitly to the spawned server process", "question_date": "2025-07-01", "sessions": [{"session_id": "session_017_a", "date": "2025-06-21", "conversation": [{"role": "user", "content": "MCP tool calls fail with HTTP 401 in clean environments after server restart.", "turn_id": 1}, {"role": "assistant", "content": "What does the server startup log show?", "turn_id": 2}, {"role": "user", "content": "The lifecycle.py start_server() was spawning server.py without passing MUNINN_AUTH_TOKEN to the child process. Each process generated its own random token. Token mismatch causes 401.", "turn_id": 3}]}, {"session_id": "session_017_b", "date": "2025-06-22", "conversation": [{"role": "user", "content": "Fixed auth propagation: start_server() now reads MUNINN_AUTH_TOKEN from environment and explicitly passes it to the spawned server process via the env argument.", "turn_id": 1}, {"role": "assistant", "content": "Critical operational fix. Environment token inheritance is essential for process spawn reliability.", "turn_id": 2}]}]}
{"question_id": "lme_syn_018", "question_type": "multi-session-qa", "question": "What two metrics does the LongMemEval adapter compute?", "expected_answer": "nDCG@10 and Recall@10", "question_date": "2025-07-01", "sessions": [{"session_id": "session_018_a", "date": "2025-06-23", "conversation": [{"role": "user", "content": "What metrics should the LongMemEval adapter report?", "turn_id": 1}, {"role": "assistant", "content": "The original paper uses specific IR metrics.", "turn_id": 2}, {"role": "user", "content": "We're implementing nDCG@10 (normalized discounted cumulative gain) and Recall@10 as the primary metrics. These match the original LongMemEval paper methodology.", "turn_id": 3}]}, {"session_id": "session_018_b", "date": "2025-06-24", "conversation": [{"role": "user", "content": "Both metrics confirmed working. The adapter reports mean_ndcg_at_k and mean_recall_at_k across all question cases.", "turn_id": 1}, {"role": "assistant", "content": "nDCG@10 and Recall@10 provide complementary perspectives on retrieval quality.", "turn_id": 2}]}]}
{"question_id": "lme_syn_019", "question_type": "temporal", "question": "When did the user complete the project-scoped memory feature (Phase 14)?", "expected_answer": "2026-02-19", "question_date": "2025-07-01", "sessions": [{"session_id": "session_019_a", "date": "2025-06-28", "conversation": [{"role": "user", "content": "Phase 14 is complete. The project-scoped memory feature is done.", "turn_id": 1}, {"role": "assistant", "content": "Congratulations! When was that wrapped up?", "turn_id": 2}, {"role": "user", "content": "We completed Phase 14 on 2026-02-19. PR #43 merged that day with 694 tests passing and 43 new scope isolation tests.", "turn_id": 3}]}]}
{"question_id": "lme_syn_020", "question_type": "temporal", "question": "When did Phase 16 complete?", "expected_answer": "2026-02-19", "question_date": "2025-07-01", "sessions": [{"session_id": "session_020_a", "date": "2025-06-29", "conversation": [{"role": "user", "content": "Phase 16 is done.", "turn_id": 1}, {"role": "assistant", "content": "Excellent! What was the completion date?", "turn_id": 2}, {"role": "user", "content": "Phase 16 completed on 2026-02-19. All 788 tests pass, the SOTA+ signed verdict infrastructure is complete with HMAC-SHA256 provenance, and PR #45 is ready for merge.", "turn_id": 3}]}]}
{"question_id": "lme_syn_021", "question_type": "temporal", "question": "In what month did the user first start working on ColBERT multi-vector storage?", "expected_answer": "Phase 13, implemented with 651 passing tests", "question_date": "2025-07-01", "sessions": [{"session_id": "session_021_a", "date": "2025-06-15", "conversation": [{"role": "user", "content": "When did we land the ColBERT multi-vector storage feature?", "turn_id": 1}, {"role": "assistant", "content": "Tell me about the timeline.", "turn_id": 2}, {"role": "user", "content": "ColBERT multi-vector storage was Phase 13 work. We implemented native MaxSim scoring in Qdrant using MultiVectorConfig. Merged PR #42 with 651 tests passing.", "turn_id": 3}]}]}
{"question_id": "lme_syn_022", "question_type": "temporal", "question": "What version was Muninn at when Phase 9 completed?", "expected_answer": "v3.6.x (consolidation and NLI integrity)", "question_date": "2025-07-01", "sessions": [{"session_id": "session_022_a", "date": "2025-06-16", "conversation": [{"role": "user", "content": "Reviewing our phase history. What version was active during Phase 9?", "turn_id": 1}, {"role": "assistant", "content": "The version history tracks closely with phase numbers.", "turn_id": 2}, {"role": "user", "content": "Phase 9 was the consolidation and NLI integrity phase. It ran during the v3.6.x era. Phase 9 focused on ConsolidationDaemon correctness and semantic deduplication.", "turn_id": 3}]}]}
{"question_id": "lme_syn_023", "question_type": "temporal", "question": "When was the MCP naming fix applied to rename global-memory to muninn?", "expected_answer": "During Phase 14/15 work, the MCP server was re-registered as muninn replacing global-memory", "question_date": "2025-07-01", "sessions": [{"session_id": "session_023_a", "date": "2025-06-20", "conversation": [{"role": "user", "content": "The MCP tools were showing as mcp__global-memory__ instead of mcp__muninn__.", "turn_id": 1}, {"role": "assistant", "content": "That's a registration naming issue.", "turn_id": 2}, {"role": "user", "content": "Fixed during Phase 14/15 transition. Ran claude mcp remove global-memory then claude mcp add -s user muninn. Tools now appear as mcp__muninn__ correctly.", "turn_id": 3}]}]}
{"question_id": "lme_syn_024", "question_type": "temporal", "question": "How long has the Muninn project been in active development phases?", "expected_answer": "From Phase 9 through Phase 16, spanning approximately phases 9-16 with continuous version increments from v3.6.x to v3.13.0", "question_date": "2025-07-01", "sessions": [{"session_id": "session_024_a", "date": "2025-06-30", "conversation": [{"role": "user", "content": "How many development phases has Muninn gone through?", "turn_id": 1}, {"role": "assistant", "content": "The SOTA+ plan tracks this carefully.", "turn_id": 2}, {"role": "user", "content": "We've completed Phases 9 through 16. That's 8 major phases from v3.6.x to v3.13.0. Each phase added a major capability: security, scoping, retrieval, project isolation, operational hardening, signed verdicts.", "turn_id": 3}]}]}
{"question_id": "lme_syn_025", "question_type": "adversarial", "question": "What database does Muninn use for vector storage?", "expected_answer": "Qdrant", "question_date": "2025-07-01", "sessions": [{"session_id": "session_025_a", "date": "2025-06-10", "conversation": [{"role": "user", "content": "I was thinking of using Pinecone for the vector database.", "turn_id": 1}, {"role": "assistant", "content": "Pinecone is cloud-based, not local-first.", "turn_id": 2}, {"role": "user", "content": "Right, we explicitly rejected Pinecone and Weaviate for being cloud-dependent. Our final decision was Qdrant for all vector storage operations. It runs locally with no cloud dependency.", "turn_id": 3}, {"role": "assistant", "content": "Qdrant is the right choice for local-first vector storage.", "turn_id": 4}]}]}
{"question_id": "lme_syn_026", "question_type": "adversarial", "question": "What language is the Muninn server written in?", "expected_answer": "Python", "question_date": "2025-07-01", "sessions": [{"session_id": "session_026_a", "date": "2025-06-11", "conversation": [{"role": "user", "content": "Someone suggested rewriting the server in Rust for performance.", "turn_id": 1}, {"role": "assistant", "content": "That would be a significant undertaking.", "turn_id": 2}, {"role": "user", "content": "We considered it but the team's strength is Python. The Muninn server is written in Python using FastAPI. We use uvicorn as the ASGI server. Rust rewrite is not planned.", "turn_id": 3}, {"role": "assistant", "content": "Python with FastAPI and uvicorn is production-proven and team-efficient.", "turn_id": 4}]}]}
{"question_id": "lme_syn_027", "question_type": "adversarial", "question": "What is the name of the Muninn project's test file for Phase 16 features?", "expected_answer": "test_v3_13_0_sota_verdict_v1.py", "question_date": "2025-07-01", "sessions": [{"session_id": "session_027_a", "date": "2025-06-12", "conversation": [{"role": "user", "content": "Previous phases used different test naming schemes before we standardized.", "turn_id": 1}, {"role": "assistant", "content": "What was the previous naming convention?", "turn_id": 2}, {"role": "user", "content": "Old tests used phase numbers like test_phase9.py. Starting with Phase 10 we standardized on test_v{version}_{feature}.py. Phase 16 tests are in test_v3_13_0_sota_verdict_v1.py.", "turn_id": 3}, {"role": "assistant", "content": "Versioned test filenames make regression tracking much cleaner.", "turn_id": 4}]}]}
{"question_id": "lme_syn_028", "question_type": "entity-centric", "question": "What is the role and significance of KuzuDB in the Muninn system?", "expected_answer": "KuzuDB is the embedded graph database used for graph memory chains, storing PRECEDES and CAUSES relations between memories", "question_date": "2025-07-01", "sessions": [{"session_id": "session_028_a", "date": "2025-06-22", "conversation": [{"role": "user", "content": "Explain how KuzuDB fits into the Muninn architecture.", "turn_id": 1}, {"role": "assistant", "content": "Tell me what it's connected to.", "turn_id": 2}, {"role": "user", "content": "KuzuDB is the embedded graph database for our memory chains feature. It stores PRECEDES and CAUSES relations between memories, enabling causal and temporal reasoning. The kuzu_v12 directory holds graph data. It's controlled by the MUNINN_CHAINS_ENABLED feature flag.", "turn_id": 3}]}]}
{"question_id": "lme_syn_029", "question_type": "entity-centric", "question": "What are MemoryRecord's two scope values and what do they mean?", "expected_answer": "scope='project' (visible only within its originating project) and scope='global' (always visible regardless of current project)", "question_date": "2025-07-01", "sessions": [{"session_id": "session_029_a", "date": "2025-06-23", "conversation": [{"role": "user", "content": "Explain the MemoryRecord scope field introduced in Phase 14.", "turn_id": 1}, {"role": "assistant", "content": "How does the scoping work?", "turn_id": 2}, {"role": "user", "content": "MemoryRecord has scope: Literal['project', 'global']. scope='project' means the memory is visible only within its originating project — it never crosses project boundaries. scope='global' means the memory is always visible regardless of current project, used for universal preferences and rules.", "turn_id": 3}]}]}
{"question_id": "lme_syn_030", "question_type": "entity-centric", "question": "What Python package provides the BM25 keyword search component in Muninn?", "expected_answer": "The BM25 store uses rank-bm25 or similar implementation integrated into the hybrid retriever for keyword-based sparse search", "question_date": "2025-07-01", "sessions": [{"session_id": "session_030_a", "date": "2025-06-24", "conversation": [{"role": "user", "content": "How is BM25 keyword search implemented in Muninn?", "turn_id": 1}, {"role": "assistant", "content": "What's the architecture?", "turn_id": 2}, {"role": "user", "content": "We have a BM25Store class that provides keyword-based sparse search. It integrates with the HybridRetriever alongside vector (Qdrant) and ColBERT (multi-vector) retrieval. BM25 results are fused using reciprocal rank fusion. The BM25 index also receives user_id and namespace for tenant isolation.", "turn_id": 3}]}]}
